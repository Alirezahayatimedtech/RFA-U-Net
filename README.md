# RFA-U-Net: RETFound Attention U-Net for OCT Choroid Segmentation

**RFA-U-Net** is a deep learning model to segment the **choroid** in Optical Coherence Tomography (OCT) images. It uses a **RETFound**-pretrained Vision Transformer (ViT) encoder and an **Attention U-Net** decoder, trained with Tversky/Dice losses and evaluated via Dice scores and micrometer-scale boundary errors.

**Last updated: 2025-11-13**
ğŸ†• New: **Segmentation-only mode** for unlabeled OCT images via `--segment_dir` (no masks needed).

---

## ğŸš€ Key Features

* **Encoder**: RETFound MAE ViT backbone
* **Decoder**: Attention U-Net with gated skip connections
* **Losses**: Tversky + Dice to handle class imbalance
* **Metrics**:

  * Dice score (overall & choroid-only)
  * Signed/Unsigned boundary errors (Î¼m)
* **Visualization**: Four-panel view & overlay boundary plots
* **ğŸ†• Segmentation-only inference**:

  * Run on **unlabeled OCT images** using `--segment_dir`
  * Saves **binary masks** and optional **boundary overlays** to disk

---

## ğŸ“ Repo Structure

```text
RFA-U-Net/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ rfa_u_net.py          # Main train / infer / segment-only script
â”‚   â”œâ”€â”€ models_vit.py         # RETFound ViT implementation
â”‚   â””â”€â”€ util/
â”‚       â””â”€â”€ pos_embed.py      # Positional embedding utils
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ visualization.ipynb   # Qualitative demo notebook
â”œâ”€â”€ weights/
â”‚   â””â”€â”€ README.md             # How to download pretrained weights
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md                 # You are here
```

---

## âš™ï¸ Requirements

* Python 3.8+
* PyTorch 1.9+
* NVIDIA GPU recommended

See `requirements.txt` for full list:

```text
torch>=1.9.0
torchvision>=0.10.0
numpy>=1.19.0
matplotlib>=3.3.0
scikit-learn>=0.24.0
timm>=0.4.12
huggingface-hub>=0.14.1
gdown>=4.7.1
Pillow>=8.0.0
rarfile>=4.0
```

---

## ğŸ›  Installation

```bash
git clone https://github.com/Alirezahayatimedtech/RFA-U-Net.git
cd RFA-U-Net
pip install -r requirements.txt
```

### ğŸ” Hugging Face Authentication

If you plan to load **RETFound** weights from HF Hub, set your token:

```bash
export HUGGINGFACE_HUB_TOKEN="hf_yourTokenHere"
# or
huggingface-cli login
```

---

## ğŸ“‚ Dataset Format (for Training / Test with Masks)

```text
data/
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ sample1.jpg
â”‚   â””â”€â”€ sample2.jpg
â””â”€â”€ masks/
    â”œâ”€â”€ sample1.png
    â””â”€â”€ sample2.png
```

* **Images**: RGB OCT scans (`.jpg`, `.png`, `.tif`, â€¦)
* **Masks**: Binary masks (`.png`) matching the image filenames (background/choroid as 2-channel one-hot internally)

---

## ğŸ¯ External-Data-Only Testing (with Masks)

Evaluate pre-trained RFA-U-Net on your own **labeled** data (no further training):

```bash
python src/rfa_u_net.py \
  --test_only \
  --test_image_dir path/to/images \
  --test_mask_dir  path/to/masks \
  --weights_type rfa-unet \
  --threshold 0.5 \
  --pixel_size_micrometers 12.5
```

Key arguments:

* `--test_only`: skip training, run evaluation only
* `--threshold`: binarization cutoff (default `0.5`)
* `--pixel_size_micrometers`: Î¼m/pixel for boundary error computation (default `10.35`)

**Sample output**:

```text
Choroid Dice on external data: 0.9523
Upper signed/unsigned error: -0.85/5.90 Î¼m
Lower signed/unsigned error:  1.12/20.50 Î¼m
```

---

## ğŸ§  Training & Inference (with Masks)

### 1ï¸âƒ£ Train from scratch (no pre-training)

```bash
python src/rfa_u_net.py \
  --image_dir data/images \
  --mask_dir  data/masks \
  --weights_type none \
  --num_epochs 30 \
  --batch_size 4
```

### 2ï¸âƒ£ Fine-tune with RETFound backbone

```bash
python src/rfa_u_net.py \
  --image_dir data/images \
  --mask_dir  data/masks \
  --weights_type retfound \
  --num_epochs 20 \
  --batch_size 8
```

This will:

* Download **RETFound** weights from HF Hub if missing
* Initialize the ViT encoder from RETFound
* Train the Attention U-Net decoder on your dataset

### 3ï¸âƒ£ Fine-tune with pre-trained RFA-U-Net weights

(Default downloads via `gdown` if not present)

```bash
python src/rfa_u_net.py \
  --image_dir data/images \
  --mask_dir  data/masks \
  --weights_type rfa-unet \
  --num_epochs 15 \
  --batch_size 8
```

---

## ğŸ†• Segmentation-Only Mode (No Masks Required)

This mode runs **pure inference** on *unlabeled* OCT images: it loads a trained model and saves segmentation masks (and optional overlays) to disk.

### CLI Example

```bash
python src/rfa_u_net.py \
  --segment_dir path/to/unlabeled_images \
  --weights_type rfa-unet \
  --weights_path weights/best_rfa_unet.pth \
  --output_dir segment_results \
  --batch_size 4 \
  --threshold 0.5 \
  --save_overlay
```

**Arguments specific to segmentation-only:**

* `--segment_dir` **(required for this mode)**

  * Path to a folder with OCT images (no masks needed)
  * Supported extensions: `.jpg`, `.jpeg`, `.png`, `.tif`, `.tiff`
* `--weights_type`

  * Recommended: `rfa-unet` (use a trained RFA-U-Net checkpoint)
  * Can also use `retfound` if you trained a custom checkpoint
* `--weights_path`

  * Path to the `.pth` checkpoint (defaults to `weights/best_rfa_unet.pth`)
* `--output_dir`

  * Base directory where results are saved (default: `segment_results`)
* `--threshold`

  * Probability cutoff for binarizing choroid predictions (default: `0.5`)
* `--save_overlay`

  * If set, saves RGB overlays with choroid boundaries drawn on the original image

**What gets saved:**

* `output_dir/masks/`

  * `_mask.png` files: binary choroid masks resized to each imageâ€™s **original size**
* `output_dir/overlays/` (only if `--save_overlay` is used)

  * `_overlay.png` files: original image with choroid **boundaries** drawn in color

The script prints per-image debug info (prediction stats, resize shapes) and warns if a mask is all black (no pixels above threshold).

---

## ğŸ“¥ Download Link for RFA-U-Net Pretrained Weights

### URL for pretrained weights (Google Drive link)

```text
RFA_UNET_WEIGHTS_URL = "https://drive.google.com/uc?export=download&id=1zDEdAmNwNK8I-QEa6fqL5E3WjDn7Z-__"
```

The script will automatically download to:

```text
weights/best_rfa_unet.pth
```

if missing and `--weights_type rfa-unet` is used.

---

## ğŸ“Š Results Snapshot

| Metric                    | Value |
| ------------------------- | ----- |
| Dice Score (choroid)      | ~0.95 |
| Upper Signed Error (Î¼m)   | ~â€“0.9 |
| Upper Unsigned Error (Î¼m) | ~6.0  |
| Lower Signed Error (Î¼m)   | ~1.1  |
| Lower Unsigned Error (Î¼m) | ~21.4 |

---

## ğŸ–¼ Example Outputs

* **Four-panel visualization**:

  * Original OCT
  * Ground-truth mask
  * Predicted mask
  * Boundary overlay (true vs predicted; upper/lower boundaries)

![Boundary overlay example](examples/sample_output.png)

Segmentation-only mode also creates overlay PNGs directly from your unlabeled OCTs (if `--save_overlay` is enabled).

---

## ğŸ“ Changelog

* **2025-11-13**

  * Added **segmentation-only mode** via `--segment_dir`
  * New output utilities:

    * Save per-image masks and overlays
    * Preserve original image resolution for saved masks
  * Updated README to document segmentation-only usage

---

## ğŸ“¬ Contact & Citation

Hayati *et al.* â€œRFA-U-Net: Choroid Segmentation in OCT with RETFound Attention U-Net,â€ *medRxiv* (2025).
DOI: `10.1101/2025.05.03.25326923`

For issues or questions, open a GitHub Issue or email **[alirezahayati17@yahoo.com](mailto:alirezahayati17@yahoo.com)**.

---

## ğŸ‘¨â€ğŸ’» Contributors

* **Alireza Hayati** â€“ Lead developer
* **Roya Arian** â€“ Mentor
* **Narges Sa** â€“ Mentor

MIT License. See [LICENSE](LICENSE).
