
# RFA-U-Net: RETFound Attention U-Net for OCT Choroid Segmentation

**RFA-U-Net** is a deep learning model designed to segment the **choroid** in Optical Coherence Tomography (OCT) images. It combines a **Vision Transformer (ViT)** encoder pre-trained with **RETFound weights** and an **Attention U-Net** decoder. The model is optimized using **Tversky** and **Dice** loss functions and evaluated using metrics such as **Dice scores** and **boundary errors** (in micrometers).

---

## ğŸš€ Features

* **Encoder**: RETFound Vision Transformer for robust feature extraction
* **Decoder**: Attention U-Net with skip connections for precise segmentation
* **Loss Functions**: Tversky and Dice loss to handle class imbalance
* **Evaluation Metrics**: Dice score, signed/unsigned boundary errors (Î¼m)
* **Visualization**: Boundary overlays of true and predicted masks

---

## ğŸ“ Repository Structure

```
RFA-U-Net/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ rfa-u-net.py          # Main model, training, and evaluation script
â”‚   â””â”€â”€ util/
â”‚       â””â”€â”€ pos_embed.py      # Positional embedding interpolation utility
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ sample_image.jpg      # Example OCT image
â”‚   â”œâ”€â”€ sample_mask.png       # Corresponding mask
â”‚   â”œâ”€â”€ sample_output.png     # Sample segmentation result
â”‚   â””â”€â”€ visualization.ipynb   # Notebook for visualizing predictions
â”œâ”€â”€ weights/
â”‚   â”œâ”€â”€ rfa_unet_best.pth     # Best pre-trained model
â”‚   â””â”€â”€ README.md             # Info about pre-trained weights
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## âš™ï¸ Prerequisites

* Python 3.8 or higher
* PyTorch 1.9.0 or higher
* NVIDIA GPU (recommended)
* All dependencies listed in `requirements.txt`

---

## ğŸ›  Installation

Clone the repository:

```bash
git clone https://github.com/Alirezahayatimedtech/RFA-U-Net.git
cd RFA-U-Net
```

Install dependencies:

```bash
pip install -r requirements.txt
```

---

## ğŸ“¦ Pre-trained Weights

The pre-trained model weights (`rfa_unet_best.pth`) will be **automatically downloaded** to the `weights/` directory the first time you run the script (`src/rfa-u-net.py`). These are the best weights saved after training on an OCT dataset.

See [`weights/README.md`](weights/README.md) for more details.

---

## ğŸ“‚ Dataset Structure

The model expects data in the following format:

```
data/
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ image1.jpg
â”‚   â”œâ”€â”€ image2.tif
â”‚   â””â”€â”€ ...
â””â”€â”€ masks/
    â”œâ”€â”€ image1_mask.png
    â”œâ”€â”€ image2_mask.png
    â””â”€â”€ ...
```

* **Images**: RGB OCT images (3 channels)
* **Masks**: Binary masks (1 channel) representing the choroid

> **Note**: Due to privacy concerns, the dataset is not included. Users must provide their own OCT dataset.

---

## ğŸ§  Training the Model

Prepare your dataset in the `data/` directory as shown above.

Then run the script with:

```bash
python src/rfa-u-net.py --image_dir path/to/your/data/images \
                        --mask_dir path/to/your/data/masks \
                        --weights_path weights/rfa_unet_best.pth
```

The script will:

* Automatically download weights (if not already present)
* Train the model for 50 epochs
* Evaluate on the validation set
* Visualize the first batch of predictions using `plot_boundaries`

---

### ğŸ”§ Available Command-Line Arguments

```
--image_dir       Path to image folder (default: data/images)
--mask_dir        Path to mask folder (default: data/masks)
--weights_path    Path to weights file (default: weights/rfa_unet_best.pth)
--image_size      Input size (default: 224)
--num_epochs      Number of training epochs (default: 50)
--batch_size      Batch size (default: 8)
```

Example with custom settings:

```bash
python src/rfa-u-net.py --image_dir my_data/images \
                        --mask_dir my_data/masks \
                        --weights_path weights/rfa_unet_best.pth \
                        --image_size 256 \
                        --num_epochs 30 \
                        --batch_size 4
```

---

## ğŸ§ª Inference with Pre-trained Weights

To use the pre-trained RFA-U-Net model for inference:

1. Ensure `rfa_unet_best.pth` is downloaded to `weights/` (it will download automatically if missing).
2. Open `examples/visualization.ipynb`.
3. Update the notebook to load your own image/mask and the weights file.
4. Run the notebook to see visual results.

---

## ğŸ“Š Visualizing Results

The notebook will generate:

* âœ… Original image
* âœ… Ground truth mask
* âœ… Predicted mask
* âœ… Boundary overlays:

  * Red = True upper
  * Green = True lower
  * Blue = Predicted upper
  * Yellow = Predicted lower

---

## ğŸ“ˆ Results

* **Test Dice Score**: \~0.95
* **Boundary Errors**:

  * Upper Signed Error: \~-0.89 Î¼m
  * Upper Unsigned Error: \~6.04 Î¼m
  * Lower Signed Error: \~1.05 Î¼m
  * Lower Unsigned Error: \~21.4 Î¼m

---

## ğŸ–¼ Example Output

*(Insert your image here if available)*

---

## ğŸ“„ License

This project is licensed under the **MIT License**. See the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

* RETFound weights and Vision Transformer implementation from [RETFound\_MAE](https://github.com/rmaphoh/RETFound_MAE)
* Inspired by Vision Transformer and U-Net architectures

---

## ğŸ“¬ Contact

For questions or issues, open an issue on GitHub or contact
ğŸ“§ **[alirezahayati17@yahoo.com](mailto:alirezahayati17@yahoo.com)**

---

## ğŸ¤ Contributors

Thanks to the following contributors:

* **Alireza Hayati** â€“ Initial development
* **Roya Arian** â€“ Helped with training experiments
* **Narges Sa** â€“ Helped with training experiments

---

## ğŸ“Œ Tags / Hashtags

\#DeepLearning #MedicalImaging #OCT #ChoroidSegmentation #VisionTransformer
\#UNet #RETFound #ComputerVision #MedicalAI #SemanticSegmentation
\#PyTorch #AIinHealthcare #ImageSegmentation #BiomedicalImaging

---

Let me know if you'd like this in a `.md` file or want to embed a Colab badge!
